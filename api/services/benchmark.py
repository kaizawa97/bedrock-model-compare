"""
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è‡ªå‹•å®Ÿè¡Œ & ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è‡ªå‹•è©•ä¾¡ã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
"""
import asyncio
import time
from typing import Dict, List, Optional
from datetime import datetime
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import json

from .pricing import MODEL_PRICING, calculate_cost, estimate_tokens
from .bedrock_executor import BedrockParallelExecutor


@dataclass
class BenchmarkTask:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¿ã‚¹ã‚¯å®šç¾©"""
    id: str
    name: str
    category: str
    prompt: str
    expected_capabilities: List[str]
    difficulty: str = "medium"  # easy, medium, hard
    max_tokens: int = 500


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    task_id: str
    model_id: str
    success: bool
    output: str
    latency_seconds: float
    input_tokens: int
    output_tokens: int
    cost_usd: float
    timestamp: datetime
    error: Optional[str] = None
    quality_score: Optional[float] = None  # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
    quality_feedback: Optional[str] = None  # å“è³ªè©•ä¾¡ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯


class BenchmarkSuite:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ"""
    
    # å“è³ªè©•ä¾¡ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    QUALITY_EVAL_PROMPT = """ã‚ãªãŸã¯å›ç­”å“è³ªã®è©•ä¾¡è€…ã§ã™ã€‚ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹å›ç­”ã‚’0-100ç‚¹ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚

ã€ã‚¿ã‚¹ã‚¯ã€‘
{task_prompt}

ã€å›ç­”ã€‘
{response}

ã€æ¡ç‚¹åŸºæº–ã€‘
- æ­£ç¢ºæ€§ï¼ˆ40ç‚¹ï¼‰: å›ç­”ãŒæ­£ç¢ºã§äº‹å®Ÿã«åŸºã¥ã„ã¦ã„ã‚‹ã‹
- å®Œå…¨æ€§ï¼ˆ30ç‚¹ï¼‰: è³ªå•ã«å¯¾ã—ã¦ååˆ†ã«ç­”ãˆã¦ã„ã‚‹ã‹
- æ˜ç­æ€§ï¼ˆ30ç‚¹ï¼‰: å›ç­”ãŒæ˜ç¢ºã§ç†è§£ã—ã‚„ã™ã„ã‹

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{{"score": <0-100ã®æ•´æ•°>, "feedback": "<ç°¡æ½”ãªè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆ>"}}"""

    # è©•ä¾¡ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«ï¼‰
    EVALUATOR_MODEL = "us.amazon.nova-lite-v1:0"
    
    # æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¿ã‚¹ã‚¯
    STANDARD_TASKS = [
        BenchmarkTask(
            id="simple_qa_1",
            name="ã‚·ãƒ³ãƒ—ãƒ«QA: äº‹å®Ÿç¢ºèª",
            category="simple_qa",
            prompt="æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿä¸€è¨€ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
            expected_capabilities=["basic_knowledge"],
            difficulty="easy",
            max_tokens=50
        ),
        BenchmarkTask(
            id="simple_qa_2",
            name="ã‚·ãƒ³ãƒ—ãƒ«QA: è¨ˆç®—",
            category="simple_qa",
            prompt="123 + 456 = ? æ•°å­—ã®ã¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
            expected_capabilities=["basic_math"],
            difficulty="easy",
            max_tokens=20
        ),
        BenchmarkTask(
            id="code_gen_1",
            name="ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ: FizzBuzz",
            category="code_generation",
            prompt="Pythonã§1ã‹ã‚‰100ã¾ã§ã®FizzBuzzã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚",
            expected_capabilities=["code_generation", "python"],
            difficulty="easy",
            max_tokens=300
        ),
        BenchmarkTask(
            id="code_gen_2",
            name="ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ: ãƒã‚¤ãƒŠãƒªã‚µãƒ¼ãƒ",
            category="code_generation",
            prompt="Pythonã§äºŒåˆ†æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚é–¢æ•°åã¯binary_searchã¨ã—ã€ã‚½ãƒ¼ãƒˆæ¸ˆã¿ãƒªã‚¹ãƒˆã¨æ¤œç´¢å€¤ã‚’å¼•æ•°ã«å–ã‚Šã¾ã™ã€‚",
            expected_capabilities=["code_generation", "algorithm"],
            difficulty="medium",
            max_tokens=400
        ),
        BenchmarkTask(
            id="reasoning_1",
            name="è«–ç†æ¨è«–: æ•°å­¦ãƒ‘ã‚ºãƒ«",
            category="reasoning",
            prompt="Aã¯Bã‚ˆã‚ŠèƒŒãŒé«˜ãã€Cã¯Bã‚ˆã‚ŠèƒŒãŒä½ã„ã€‚Aã¨Cã§ã¯ã©ã¡ã‚‰ãŒèƒŒãŒé«˜ã„ã§ã™ã‹ï¼Ÿç†ç”±ã‚‚å«ã‚ã¦ç­”ãˆã¦ãã ã•ã„ã€‚",
            expected_capabilities=["logical_reasoning"],
            difficulty="medium",
            max_tokens=200
        ),
        BenchmarkTask(
            id="reasoning_2",
            name="è«–ç†æ¨è«–: è¤‡é›‘ãªæ¡ä»¶",
            category="reasoning",
            prompt="5äººã®å‹äººA,B,C,D,EãŒä¸€åˆ—ã«ä¸¦ã‚“ã§ã„ã¾ã™ã€‚Aã¯Bã®éš£ã€Cã¯Dã®éš£ã§ã¯ãªã„ã€Eã¯Aã®å³å´ã«ã„ã¾ã™ã€‚å¯èƒ½ãªä¸¦ã³é †ã‚’1ã¤ç¤ºã—ã¦ãã ã•ã„ã€‚",
            expected_capabilities=["complex_reasoning"],
            difficulty="hard",
            max_tokens=300
        ),
        BenchmarkTask(
            id="creative_1",
            name="å‰µé€ æ€§: çŸ­ç·¨ã‚¹ãƒˆãƒ¼ãƒªãƒ¼",
            category="brainstorming",
            prompt="ã€ŒAIã¨äººé–“ã®å‹æƒ…ã€ã‚’ãƒ†ãƒ¼ãƒã«ã€100æ–‡å­—ç¨‹åº¦ã®è¶…çŸ­ç·¨å°èª¬ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
            expected_capabilities=["creative_writing"],
            difficulty="medium",
            max_tokens=200
        ),
        BenchmarkTask(
            id="analysis_1",
            name="åˆ†æ: é•·æ‰€çŸ­æ‰€",
            category="analysis",
            prompt="ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ã®é•·æ‰€ã¨çŸ­æ‰€ã‚’å„3ã¤ãšã¤ã€ç®‡æ¡æ›¸ãã§ç°¡æ½”ã«è¿°ã¹ã¦ãã ã•ã„ã€‚",
            expected_capabilities=["analysis", "structured_output"],
            difficulty="easy",
            max_tokens=300
        ),
        BenchmarkTask(
            id="doc_1",
            name="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: é–¢æ•°èª¬æ˜",
            category="documentation",
            prompt="ä»¥ä¸‹ã®Pythoné–¢æ•°ã®docstringã‚’æ›¸ã„ã¦ãã ã•ã„:\ndef calculate_average(numbers: list) -> float:\n    return sum(numbers) / len(numbers)",
            expected_capabilities=["documentation"],
            difficulty="easy",
            max_tokens=200
        ),
        BenchmarkTask(
            id="multilingual_1",
            name="å¤šè¨€èª: ç¿»è¨³",
            category="general",
            prompt="ã€Œäººå·¥çŸ¥èƒ½ã¯ç§ãŸã¡ã®ç”Ÿæ´»ã‚’å¤§ããå¤‰ãˆã¦ã„ã¾ã™ã€ã‚’è‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚",
            expected_capabilities=["translation"],
            difficulty="easy",
            max_tokens=100
        )
    ]
    
    def __init__(self, region: str = "us-east-1"):
        self.region = region
        self.executor = BedrockParallelExecutor(region=region)
        self.results: List[BenchmarkResult] = []
    
    def get_available_tasks(self) -> List[Dict]:
        """åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯ä¸€è¦§"""
        return [
            {
                "id": t.id,
                "name": t.name,
                "category": t.category,
                "difficulty": t.difficulty,
                "expected_capabilities": t.expected_capabilities
            }
            for t in self.STANDARD_TASKS
        ]
    
    def _evaluate_quality(self, task_prompt: str, response: str) -> Dict:
        """å›ç­”ã®å“è³ªã‚’è©•ä¾¡ï¼ˆè©•ä¾¡è€…ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰"""
        if not response or len(response.strip()) == 0:
            return {"score": 0, "feedback": "å›ç­”ãªã—"}
        
        eval_prompt = self.QUALITY_EVAL_PROMPT.format(
            task_prompt=task_prompt,
            response=response[:2000]  # é•·ã™ãã‚‹å›ç­”ã¯åˆ‡ã‚Šè©°ã‚
        )
        
        try:
            eval_results = self.executor.execute_parallel_models(
                model_ids=[self.EVALUATOR_MODEL],
                prompt=eval_prompt,
                max_tokens=200,
                temperature=0.1
            )
            
            if eval_results and eval_results[0].get("success"):
                output = eval_results[0]["output"]
                # JSONã‚’æŠ½å‡º
                import re
                json_match = re.search(r'\{[^}]+\}', output)
                if json_match:
                    eval_data = json.loads(json_match.group())
                    score = max(0, min(100, int(eval_data.get("score", 50))))
                    feedback = eval_data.get("feedback", "")
                    return {"score": score, "feedback": feedback}
        except Exception as e:
            print(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        
        return {"score": 50, "feedback": "è©•ä¾¡ä¸èƒ½"}
    
    def run_benchmark(
        self,
        model_ids: List[str],
        task_ids: List[str] = None,
        categories: List[str] = None
    ) -> Dict:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        
        # ã‚¿ã‚¹ã‚¯é¸æŠ
        tasks = self.STANDARD_TASKS
        if task_ids:
            tasks = [t for t in tasks if t.id in task_ids]
        if categories:
            tasks = [t for t in tasks if t.category in categories]
        
        if not tasks:
            return {"error": "No tasks selected"}
        
        print(f"ğŸ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {len(model_ids)}ãƒ¢ãƒ‡ãƒ« Ã— {len(tasks)}ã‚¿ã‚¹ã‚¯")
        start_time = time.time()
        
        results = []
        
        for task in tasks:
            print(f"\nğŸ“‹ ã‚¿ã‚¹ã‚¯: {task.name}")
            
            # å„ãƒ¢ãƒ‡ãƒ«ã§å®Ÿè¡Œ
            task_results = self.executor.execute_parallel_models(
                model_ids=model_ids,
                prompt=task.prompt,
                max_tokens=task.max_tokens,
                temperature=0.3  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ä½æ¸©åº¦ã§
            )
            
            for result in task_results:
                cost_info = result.get("cost", {})
                
                # æˆåŠŸã—ãŸå ´åˆã¯å“è³ªè©•ä¾¡ã‚’å®Ÿè¡Œ
                quality_score = None
                quality_feedback = None
                if result["success"] and result.get("output"):
                    quality_eval = self._evaluate_quality(task.prompt, result["output"])
                    quality_score = quality_eval["score"]
                    quality_feedback = quality_eval["feedback"]
                
                benchmark_result = BenchmarkResult(
                    task_id=task.id,
                    model_id=result["model_id"],
                    success=result["success"],
                    output=result.get("output", ""),
                    latency_seconds=result["elapsed_time"],
                    input_tokens=cost_info.get("input_tokens", 0),
                    output_tokens=cost_info.get("output_tokens", 0),
                    cost_usd=cost_info.get("total_cost", 0),
                    timestamp=datetime.now(),
                    error=result.get("error"),
                    quality_score=quality_score,
                    quality_feedback=quality_feedback
                )
                results.append(benchmark_result)
                self.results.append(benchmark_result)
        
        total_time = time.time() - start_time
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self._generate_report(results, tasks, model_ids, total_time)
        
        return report
    
    def _generate_report(
        self,
        results: List[BenchmarkResult],
        tasks: List[BenchmarkTask],
        model_ids: List[str],
        total_time: float
    ) -> Dict:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥é›†è¨ˆ
        model_stats = {}
        for model_id in model_ids:
            model_results = [r for r in results if r.model_id == model_id]
            successful = [r for r in model_results if r.success]
            quality_scores = [r.quality_score for r in successful if r.quality_score is not None]
            
            model_stats[model_id] = {
                "model_name": model_id.split(".")[-1][:30],
                "total_tasks": len(model_results),
                "successful_tasks": len(successful),
                "success_rate": len(successful) / len(model_results) * 100 if model_results else 0,
                "avg_latency": sum(r.latency_seconds for r in successful) / len(successful) if successful else 0,
                "total_cost": sum(r.cost_usd for r in model_results),
                "total_tokens": sum(r.input_tokens + r.output_tokens for r in model_results),
                "avg_quality_score": sum(quality_scores) / len(quality_scores) if quality_scores else 50,
                "by_category": self._aggregate_by_category(model_results, tasks)
            }
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
        category_stats = {}
        for task in tasks:
            if task.category not in category_stats:
                category_stats[task.category] = {
                    "task_count": 0,
                    "best_model": None,
                    "best_latency": float("inf"),
                    "best_success_rate": 0
                }
            category_stats[task.category]["task_count"] += 1
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ç‰¹å®š
        for category in category_stats:
            for model_id, stats in model_stats.items():
                cat_stats = stats["by_category"].get(category, {})
                if cat_stats.get("success_rate", 0) > category_stats[category]["best_success_rate"]:
                    category_stats[category]["best_model"] = model_id
                    category_stats[category]["best_success_rate"] = cat_stats.get("success_rate", 0)
                    category_stats[category]["best_latency"] = cat_stats.get("avg_latency", 0)
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”Ÿæˆ
        rankings = self._generate_rankings(model_stats)
        
        # æ¨å¥¨äº‹é …
        recommendations = self._generate_recommendations(model_stats, category_stats)
        
        return {
            "summary": {
                "total_models": len(model_ids),
                "total_tasks": len(tasks),
                "total_executions": len(results),
                "successful_executions": sum(1 for r in results if r.success),
                "total_time_seconds": round(total_time, 2),
                "total_cost_usd": round(sum(r.cost_usd for r in results), 6),
                "timestamp": datetime.now().isoformat()
            },
            "model_performance": model_stats,
            "category_analysis": category_stats,
            "rankings": rankings,
            "recommendations": recommendations,
            "detailed_results": [
                {
                    "task_id": r.task_id,
                    "model_id": r.model_id,
                    "success": r.success,
                    "latency": round(r.latency_seconds, 2),
                    "cost": round(r.cost_usd, 6),
                    "quality_score": r.quality_score,
                    "quality_feedback": r.quality_feedback,
                    "output_preview": r.output[:100] if r.output else None,
                    "error": r.error
                }
                for r in results
            ]
        }
    
    def _aggregate_by_category(
        self, 
        results: List[BenchmarkResult], 
        tasks: List[BenchmarkTask]
    ) -> Dict:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ"""
        task_categories = {t.id: t.category for t in tasks}
        category_results = {}
        
        for result in results:
            category = task_categories.get(result.task_id, "unknown")
            if category not in category_results:
                category_results[category] = {"success": 0, "total": 0, "latency_sum": 0}
            
            category_results[category]["total"] += 1
            if result.success:
                category_results[category]["success"] += 1
                category_results[category]["latency_sum"] += result.latency_seconds
        
        return {
            cat: {
                "success_rate": data["success"] / data["total"] * 100 if data["total"] > 0 else 0,
                "avg_latency": data["latency_sum"] / data["success"] if data["success"] > 0 else 0,
                "task_count": data["total"]
            }
            for cat, data in category_results.items()
        }
    
    def _generate_rankings(self, model_stats: Dict) -> Dict:
        """ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”Ÿæˆ"""
        models = list(model_stats.items())
        
        return {
            "by_quality": sorted(
                [{"model_id": m, "value": s.get("avg_quality_score", 50)} for m, s in models],
                key=lambda x: x["value"],
                reverse=True
            ),
            "by_speed": sorted(
                [{"model_id": m, "value": s["avg_latency"]} for m, s in models if s["avg_latency"] > 0],
                key=lambda x: x["value"]
            ),
            "by_cost_efficiency": sorted(
                [{"model_id": m, "value": s["total_cost"]} for m, s in models],
                key=lambda x: x["value"]
            ),
            "overall": self._calculate_overall_ranking(model_stats)
        }
    
    def _calculate_overall_ranking(self, model_stats: Dict) -> List[Dict]:
        """ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨ˆç®—ï¼ˆé€Ÿåº¦ãƒ»ã‚³ã‚¹ãƒˆãƒ»å“è³ªã®3è»¸ï¼‰"""
        scores = []
        
        # æ­£è¦åŒ–ç”¨ã®æœ€å¤§å€¤ãƒ»æœ€å°å€¤ã‚’å–å¾—
        latencies = [s["avg_latency"] for s in model_stats.values() if s["avg_latency"] > 0]
        costs = [s["total_cost"] for s in model_stats.values()]
        qualities = [s.get("avg_quality_score", 50) for s in model_stats.values()]
        
        max_latency = max(latencies) if latencies else 10
        min_latency = min(latencies) if latencies else 0
        max_cost = max(costs) if costs else 0.01
        min_cost = min(costs) if costs else 0
        max_quality = max(qualities) if qualities else 100
        min_quality = min(qualities) if qualities else 0
        
        for model_id, stats in model_stats.items():
            # é€Ÿåº¦ã‚¹ã‚³ã‚¢: é€Ÿã„ã»ã©é«˜å¾—ç‚¹ï¼ˆ0-100ã«æ­£è¦åŒ–ï¼‰
            if max_latency > min_latency and stats["avg_latency"] > 0:
                speed_score = 100 * (1 - (stats["avg_latency"] - min_latency) / (max_latency - min_latency))
            else:
                speed_score = 100 if stats["avg_latency"] > 0 else 0
            
            # ã‚³ã‚¹ãƒˆã‚¹ã‚³ã‚¢: å®‰ã„ã»ã©é«˜å¾—ç‚¹ï¼ˆ0-100ã«æ­£è¦åŒ–ï¼‰
            if max_cost > min_cost:
                cost_score = 100 * (1 - (stats["total_cost"] - min_cost) / (max_cost - min_cost))
            else:
                cost_score = 100
            
            # å“è³ªã‚¹ã‚³ã‚¢: è©•ä¾¡è€…ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ¡ç‚¹çµæœï¼ˆ0-100ï¼‰
            quality_score = stats.get("avg_quality_score", 50)
            
            # ç·åˆã‚¹ã‚³ã‚¢: å“è³ª40% + é€Ÿåº¦30% + ã‚³ã‚¹ãƒˆ30%
            overall = (quality_score * 0.4 + speed_score * 0.3 + cost_score * 0.3)
            
            scores.append({
                "model_id": model_id,
                "model_name": stats["model_name"],
                "overall_score": round(overall),
                "quality_score": round(quality_score),
                "speed_score": round(speed_score),
                "cost_score": round(cost_score)
            })
        
        return sorted(scores, key=lambda x: x["overall_score"], reverse=True)
    
    def _generate_recommendations(self, model_stats: Dict, category_stats: Dict) -> List[Dict]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # æœ€é«˜å“è³ªãƒ¢ãƒ‡ãƒ«
        best_quality = max(model_stats.items(), key=lambda x: x[1].get("avg_quality_score", 0))
        recommendations.append({
            "type": "best_quality",
            "title": "æœ€é«˜å“è³ªãƒ¢ãƒ‡ãƒ«",
            "model_id": best_quality[0],
            "reason": f"å“è³ªã‚¹ã‚³ã‚¢ {best_quality[1].get('avg_quality_score', 0):.1f}ç‚¹"
        })
        
        # æœ€é€Ÿãƒ¢ãƒ‡ãƒ«
        fastest = min(
            [(m, s) for m, s in model_stats.items() if s["avg_latency"] > 0],
            key=lambda x: x[1]["avg_latency"],
            default=(None, None)
        )
        if fastest[0]:
            recommendations.append({
                "type": "fastest",
                "title": "æœ€é€Ÿãƒ¢ãƒ‡ãƒ«",
                "model_id": fastest[0],
                "reason": f"å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· {fastest[1]['avg_latency']:.2f}ç§’"
            })
        
        # æœ€ã‚‚ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«
        cheapest = min(model_stats.items(), key=lambda x: x[1]["total_cost"])
        recommendations.append({
            "type": "most_cost_effective",
            "title": "æœ€ã‚‚ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«",
            "model_id": cheapest[0],
            "reason": f"ç·ã‚³ã‚¹ãƒˆ ${cheapest[1]['total_cost']:.6f}"
        })
        
        # ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆå“è³ªã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
        balanced = max(
            model_stats.items(),
            key=lambda x: (x[1].get("avg_quality_score", 50) / max(x[1]["total_cost"] * 1000, 0.001))
        )
        recommendations.append({
            "type": "balanced",
            "title": "ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆå“è³ª/ã‚³ã‚¹ãƒˆæ¯”ï¼‰",
            "model_id": balanced[0],
            "reason": f"å“è³ª {balanced[1].get('avg_quality_score', 50):.0f}ç‚¹ / ã‚³ã‚¹ãƒˆ ${balanced[1]['total_cost']:.4f}"
        })
        
        return recommendations


def get_benchmark_suite(region: str = "us-east-1") -> BenchmarkSuite:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã‚’å–å¾—"""
    return BenchmarkSuite(region=region)
