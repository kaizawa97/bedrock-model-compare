"""
å£æ‰“ã¡ï¼ˆãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆ/ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ï¼‰ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
"""
import asyncio
import json
import uuid
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse

from models.requests import DebateRequest, HumanInputRequest
from services.bedrock_executor import BedrockParallelExecutor

router = APIRouter(prefix="/api", tags=["debate"])

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å¾…ã¡ç”¨ï¼‰
debate_sessions: dict[str, asyncio.Queue] = {}

MODE_PROMPTS = {
    "debate": "ã‚ãªãŸã¯ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆã®å‚åŠ è€…ã§ã™ã€‚ç›¸æ‰‹ã®æ„è¦‹ã«å¯¾ã—ã¦å»ºè¨­çš„ã«åè«–ã—ã€è‡ªåˆ†ã®ç«‹å ´ã‚’æ˜ç¢ºã«ä¸»å¼µã—ã¦ãã ã•ã„ã€‚",
    "brainstorm": "ã‚ãªãŸã¯ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ã®å‚åŠ è€…ã§ã™ã€‚ä»–ã®å‚åŠ è€…ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç™ºå±•ã•ã›ã€æ–°ã—ã„è¦–ç‚¹ã‚„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚",
    "critique": "ã‚ãªãŸã¯æ‰¹è©•å®¶ã§ã™ã€‚æç¤ºã•ã‚ŒãŸå†…å®¹ã®é•·æ‰€ã¨çŸ­æ‰€ã‚’åˆ†æã—ã€æ”¹å–„ç‚¹ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
}


@router.post("/debate")
async def execute_debate(request: DebateRequest):
    """ãƒ¢ãƒ‡ãƒ«åŒå£«ã®å£æ‰“ã¡ï¼ˆãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆ/ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ï¼‰ã‚’å®Ÿè¡Œ"""
    if len(request.model_ids) < 2:
        raise HTTPException(status_code=400, detail="å£æ‰“ã¡ã«ã¯2ã¤ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™")
    
    try:
        executor = BedrockParallelExecutor(region=request.region)
        system_prompt = MODE_PROMPTS.get(request.mode, MODE_PROMPTS["debate"])
        
        conversation_history = []
        all_results = []
        
        initial_prompt = f"""ã€ãƒˆãƒ”ãƒƒã‚¯ã€‘{request.topic}

{system_prompt}

ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€ã‚ãªãŸã®è¦‹è§£ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚"""
        
        print(f"ğŸ­ å£æ‰“ã¡é–‹å§‹: {request.mode}ãƒ¢ãƒ¼ãƒ‰, {request.rounds}ãƒ©ã‚¦ãƒ³ãƒ‰")
        print(f"   å‚åŠ ãƒ¢ãƒ‡ãƒ«: {request.model_ids}")
        print(f"   ãƒˆãƒ”ãƒƒã‚¯: {request.topic[:50]}...")
        
        loop = asyncio.get_event_loop()
        
        for round_num in range(request.rounds):
            print(f"\nğŸ“ ãƒ©ã‚¦ãƒ³ãƒ‰ {round_num + 1}/{request.rounds}")
            round_results = []
            
            for i, model_id in enumerate(request.model_ids):
                if round_num == 0 and i == 0:
                    current_prompt = initial_prompt
                else:
                    prev_statements = "\n\n".join([
                        f"ã€{r['model_id'].split('.')[-1]}ã®ç™ºè¨€ã€‘\n{r['output']}"
                        for r in conversation_history[-len(request.model_ids):]
                    ])
                    current_prompt = f"""ã€ãƒˆãƒ”ãƒƒã‚¯ã€‘{request.topic}

{system_prompt}

ã€ã“ã‚Œã¾ã§ã®è­°è«–ã€‘
{prev_statements}

ä¸Šè¨˜ã®è­°è«–ã‚’è¸ã¾ãˆã¦ã€ã‚ãªãŸã®è¦‹è§£ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚"""
                
                result = await loop.run_in_executor(
                    None,
                    lambda mid=model_id, cp=current_prompt, idx=round_num * len(request.model_ids) + i: 
                        executor.invoke_model_with_reasoning(
                            mid, cp, request.max_tokens, request.temperature, idx,
                            request.enable_reasoning, request.reasoning_budget_tokens
                        ) if request.enable_reasoning else executor.invoke_model(
                            mid, cp, request.max_tokens, request.temperature, idx
                        )
                )
                
                result["round"] = round_num + 1
                result["speaker_index"] = i
                result["skipped"] = not result["success"]  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—æ‰±ã„
                
                # æˆåŠŸã—ãŸå ´åˆã®ã¿ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆæ¬¡ã®ç™ºè¨€è€…ã®å‚ç…§ç”¨ï¼‰
                if result["success"]:
                    conversation_history.append(result)
                
                round_results.append(result)
                
                status = "âœ…" if result["success"] else "â­ï¸ ã‚¹ã‚­ãƒƒãƒ—"
                model_short = model_id.split('.')[-1][:20]
                print(f"   {status} [{model_short}]: {result['elapsed_time']:.2f}ç§’")
            
            all_results.append({
                "round": round_num + 1,
                "results": round_results
            })
        
        print(f"\nâœ¨ å£æ‰“ã¡å®Œäº†ï¼")
        
        # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        skipped_count = sum(1 for round_data in all_results for r in round_data["results"] if r.get("skipped"))
        if skipped_count > 0:
            print(f"   â­ï¸ {skipped_count}ä»¶ã®ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
        
        return {
            "mode": request.mode,
            "topic": request.topic,
            "total_rounds": request.rounds,
            "participants": request.model_ids,
            "enable_reasoning": request.enable_reasoning,
            "rounds": all_results,
            "summary": {
                "total_exchanges": len(conversation_history),
                "success_count": sum(1 for r in conversation_history if r["success"]),
                "skipped_count": skipped_count,
                "total_time": sum(r["elapsed_time"] for r in conversation_history)
            }
        }
        
    except Exception as e:
        import traceback
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/debate-stream")
async def execute_debate_stream(request: DebateRequest):
    """ãƒ¢ãƒ‡ãƒ«åŒå£«ã®å£æ‰“ã¡ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆï¼‰"""
    if len(request.model_ids) < 2:
        raise HTTPException(status_code=400, detail="å£æ‰“ã¡ã«ã¯2ã¤ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDç”Ÿæˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å‚åŠ æ™‚ã®ã¿ï¼‰
    session_id = str(uuid.uuid4()) if request.include_human else None
    if session_id:
        debate_sessions[session_id] = asyncio.Queue()
    
    async def generate():
        try:
            executor = BedrockParallelExecutor(region=request.region)
            system_prompt = MODE_PROMPTS.get(request.mode, MODE_PROMPTS["debate"])
            conversation_history = []
            skipped_count = 0  # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            
            # å‚åŠ è€…ãƒªã‚¹ãƒˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å«ã‚€å ´åˆï¼‰
            participants = list(request.model_ids)
            if request.include_human:
                participants.append("human")
            
            initial_prompt = f"""ã€ãƒˆãƒ”ãƒƒã‚¯ã€‘{request.topic}

{system_prompt}

ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€ã‚ãªãŸã®è¦‹è§£ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚"""
            
            yield f"data: {json.dumps({'type': 'start', 'mode': request.mode, 'topic': request.topic, 'total_rounds': request.rounds, 'participants': participants, 'session_id': session_id})}\n\n"
            
            loop = asyncio.get_event_loop()
            
            for round_num in range(request.rounds):
                yield f"data: {json.dumps({'type': 'round_start', 'round': round_num + 1})}\n\n"
                
                # ãƒ¢ãƒ‡ãƒ«ã®ç™ºè¨€
                for i, model_id in enumerate(request.model_ids):
                    yield f"data: {json.dumps({'type': 'speaking', 'round': round_num + 1, 'speaker_index': i, 'model_id': model_id})}\n\n"
                    
                    if round_num == 0 and i == 0:
                        current_prompt = initial_prompt
                    else:
                        prev_statements = "\n\n".join([
                            f"ã€{'ã‚ãªãŸ' if r['model_id'] == 'human' else r['model_id'].split('.')[-1]}ã®ç™ºè¨€ã€‘\n{r['output']}"
                            for r in conversation_history[-len(participants):]
                        ])
                        current_prompt = f"""ã€ãƒˆãƒ”ãƒƒã‚¯ã€‘{request.topic}

{system_prompt}

ã€ã“ã‚Œã¾ã§ã®è­°è«–ã€‘
{prev_statements}

ä¸Šè¨˜ã®è­°è«–ã‚’è¸ã¾ãˆã¦ã€ã‚ãªãŸã®è¦‹è§£ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚"""
                    
                    result = await loop.run_in_executor(
                        None,
                        lambda mid=model_id, cp=current_prompt, idx=round_num * len(participants) + i:
                            executor.invoke_model_with_reasoning(
                                mid, cp, request.max_tokens, request.temperature, idx,
                                request.enable_reasoning, request.reasoning_budget_tokens
                            ) if request.enable_reasoning else executor.invoke_model(
                                mid, cp, request.max_tokens, request.temperature, idx
                            )
                    )
                    
                    result["round"] = round_num + 1
                    result["speaker_index"] = i
                    result["skipped"] = not result["success"]  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—æ‰±ã„
                    
                    # æˆåŠŸã—ãŸå ´åˆã®ã¿ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆæ¬¡ã®ç™ºè¨€è€…ã®å‚ç…§ç”¨ï¼‰
                    if result["success"]:
                        conversation_history.append(result)
                    else:
                        skipped_count += 1
                    
                    yield f"data: {json.dumps({'type': 'speech', 'data': result})}\n\n"
                    await asyncio.sleep(0)
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ï¼ˆinclude_humanãŒTrueã®å ´åˆï¼‰
                if request.include_human and session_id:
                    human_speaker_index = len(request.model_ids)
                    yield f"data: {json.dumps({'type': 'waiting_human', 'round': round_num + 1, 'speaker_index': human_speaker_index, 'session_id': session_id})}\n\n"
                    
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å¾…ã¤ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 5åˆ†ï¼‰
                    try:
                        human_message = await asyncio.wait_for(
                            debate_sessions[session_id].get(),
                            timeout=300.0
                        )
                        
                        # ã‚¹ã‚­ãƒƒãƒ—ã®å ´åˆã¯ç™ºè¨€ã‚’è¨˜éŒ²ã—ãªã„
                        if human_message == "[ã‚¹ã‚­ãƒƒãƒ—]":
                            yield f"data: {json.dumps({'type': 'speech', 'data': {'model_id': 'human', 'output': 'ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰', 'success': True, 'elapsed_time': 0.0, 'round': round_num + 1, 'speaker_index': human_speaker_index, 'skipped': True}})}\n\n"
                        else:
                            human_result = {
                                "model_id": "human",
                                "output": human_message,
                                "success": True,
                                "elapsed_time": 0.0,
                                "round": round_num + 1,
                                "speaker_index": human_speaker_index,
                            }
                            conversation_history.append(human_result)
                            
                            yield f"data: {json.dumps({'type': 'speech', 'data': human_result})}\n\n"
                    except asyncio.TimeoutError:
                        yield f"data: {json.dumps({'type': 'error', 'message': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ'})}\n\n"
                        break
                
                yield f"data: {json.dumps({'type': 'round_end', 'round': round_num + 1})}\n\n"
            
            summary = {
                "total_exchanges": len(conversation_history),
                "success_count": sum(1 for r in conversation_history if r["success"]),
                "skipped_count": skipped_count,
                "total_time": sum(r["elapsed_time"] for r in conversation_history)
            }
            yield f"data: {json.dumps({'type': 'complete', 'summary': summary})}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
        finally:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if session_id and session_id in debate_sessions:
                del debate_sessions[session_id]
    
    return StreamingResponse(generate(), media_type="text/event-stream")


@router.post("/debate-human-input")
async def submit_human_input(request: HumanInputRequest):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’é€ä¿¡"""
    if request.session_id not in debate_sessions:
        raise HTTPException(status_code=404, detail="ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    await debate_sessions[request.session_id].put(request.message)
    return {"status": "ok"}
