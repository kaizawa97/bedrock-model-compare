"""
æŒ‡æ®è€…ãƒ¢ãƒ¼ãƒ‰ï¼ˆConductor Modeï¼‰ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
"""
import asyncio
import json
import re
from fastapi import APIRouter, HTTPException

from models.requests import ConductorRequest
from services.bedrock_executor import BedrockParallelExecutor

router = APIRouter(prefix="/api", tags=["conductor"])


@router.post("/conductor")
async def execute_conductor(request: ConductorRequest):
    """
    æŒ‡æ®è€…ãƒ¢ãƒ¼ãƒ‰ï¼š1ã¤ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆæŒ‡æ®è€…ï¼‰ãŒä»–ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰ã«æŒ‡ç¤ºã‚’å‡ºã™
    
    ãƒ¢ãƒ¼ãƒ‰:
    - delegate: æŒ‡æ®è€…ãŒã‚¿ã‚¹ã‚¯ã‚’åˆ†å‰²ã—ã€å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ç•°ãªã‚‹ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’å‰²ã‚Šå½“ã¦
    - evaluate: å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã«åŒã˜ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã•ã›ã€æŒ‡æ®è€…ãŒçµæœã‚’è©•ä¾¡ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    - synthesize: å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å›ç­”ã‚’æŒ‡æ®è€…ãŒçµ±åˆã—ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆ
    """
    if len(request.worker_model_ids) < 1:
        raise HTTPException(status_code=400, detail="å°‘ãªãã¨ã‚‚1ã¤ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™")
    
    try:
        executor = BedrockParallelExecutor(region=request.region)
        loop = asyncio.get_event_loop()
        
        print(f"ğŸ¼ æŒ‡æ®è€…ãƒ¢ãƒ¼ãƒ‰é–‹å§‹: {request.mode}")
        print(f"   æŒ‡æ®è€…: {request.conductor_model_id}")
        print(f"   ãƒ¯ãƒ¼ã‚«ãƒ¼: {len(request.worker_model_ids)}å€‹")
        print(f"   ã‚¿ã‚¹ã‚¯: {request.task[:50]}...")
        
        result = {
            "mode": request.mode,
            "conductor_model": request.conductor_model_id,
            "worker_models": request.worker_model_ids,
            "original_task": request.task,
            "phases": []
        }
        
        if request.mode == "delegate":
            result = await _execute_delegate_mode(request, executor, loop, result)
        elif request.mode == "evaluate":
            result = await _execute_evaluate_mode(request, executor, loop, result)
        elif request.mode == "synthesize":
            result = await _execute_synthesize_mode(request, executor, loop, result)
        
        # ã‚µãƒãƒªãƒ¼è¨ˆç®—
        all_results = []
        for phase in result["phases"]:
            if "results" in phase:
                all_results.extend(phase["results"])
            if "conductor_response" in phase:
                all_results.append(phase["conductor_response"])
        
        result["summary"] = {
            "total_calls": len(all_results),
            "success_count": sum(1 for r in all_results if r.get("success")),
            "total_time": sum(r.get("elapsed_time", 0) for r in all_results),
            "total_cost": sum(r.get("cost", {}).get("total_cost", 0) for r in all_results if r.get("success"))
        }
        
        print(f"\nâœ¨ æŒ‡æ®è€…ãƒ¢ãƒ¼ãƒ‰å®Œäº†ï¼")
        return result
        
    except Exception as e:
        import traceback
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


async def _execute_delegate_mode(request, executor, loop, result):
    """ã‚¿ã‚¹ã‚¯åˆ†å‰²ãƒ¢ãƒ¼ãƒ‰"""
    # Phase 1: æŒ‡æ®è€…ãŒã‚¿ã‚¹ã‚¯ã‚’åˆ†å‰²
    delegate_prompt = f"""ã‚ãªãŸã¯è¤‡æ•°ã®AIãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡æ®ã™ã‚‹ã€ŒæŒ‡æ®è€…ã€ã§ã™ã€‚
ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’{len(request.worker_model_ids)}å€‹ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®ã‚¿ã‚¹ã‚¯ã€‘
{request.task}

ã€ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¢ãƒ‡ãƒ«æ•°ã€‘
{len(request.worker_model_ids)}å€‹

ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
```json
{{
  "subtasks": [
    {{"id": 1, "task": "ã‚µãƒ–ã‚¿ã‚¹ã‚¯1ã®å†…å®¹", "focus": "ã“ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®ç„¦ç‚¹"}},
    {{"id": 2, "task": "ã‚µãƒ–ã‚¿ã‚¹ã‚¯2ã®å†…å®¹", "focus": "ã“ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®ç„¦ç‚¹"}}
  ],
  "integration_strategy": "çµæœã‚’ã©ã†çµ±åˆã™ã‚‹ã‹ã®èª¬æ˜"
}}
```"""
    
    print(f"\nğŸ“‹ Phase 1: ã‚¿ã‚¹ã‚¯åˆ†å‰²ä¸­...")
    conductor_result = await loop.run_in_executor(
        None,
        executor.invoke_model,
        request.conductor_model_id,
        delegate_prompt,
        request.max_tokens,
        0.3,
        0
    )
    
    result["phases"].append({
        "phase": "task_delegation",
        "conductor_response": conductor_result
    })
    
    if not conductor_result["success"]:
        return result
    
    # JSONã‚’æŠ½å‡º
    json_match = re.search(r'```json\s*(.*?)\s*```', conductor_result["output"], re.DOTALL)
    if json_match:
        try:
            subtasks_data = json.loads(json_match.group(1))
            subtasks = subtasks_data.get("subtasks", [])
        except:
            subtasks = [{"id": i+1, "task": request.task} for i in range(len(request.worker_model_ids))]
    else:
        subtasks = [{"id": i+1, "task": request.task} for i in range(len(request.worker_model_ids))]
    
    # Phase 2: å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã•ã›ã‚‹
    print(f"\nğŸ”§ Phase 2: ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè¡Œä¸­...")
    worker_results = []
    for i, (model_id, subtask) in enumerate(zip(request.worker_model_ids, subtasks)):
        task_content = subtask.get("task", request.task) if isinstance(subtask, dict) else subtask
        worker_prompt = f"""ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

{task_content}

è©³ç´°ã‹ã¤å…·ä½“çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
        
        worker_result = await loop.run_in_executor(
            None,
            executor.invoke_model,
            model_id,
            worker_prompt,
            request.max_tokens,
            request.temperature,
            i
        )
        worker_result["assigned_subtask"] = subtask
        worker_results.append(worker_result)
        
        status = "âœ…" if worker_result["success"] else "âŒ"
        print(f"   {status} Worker {i+1}: {model_id.split('.')[-1][:20]}")
    
    result["phases"].append({
        "phase": "worker_execution",
        "results": worker_results
    })
    
    # Phase 3: æŒ‡æ®è€…ãŒçµæœã‚’çµ±åˆ
    print(f"\nğŸ¯ Phase 3: çµæœçµ±åˆä¸­...")
    worker_outputs = "\n\n".join([
        f"ã€ãƒ¯ãƒ¼ã‚«ãƒ¼{i+1} ({r['model_id'].split('.')[-1][:20]})ã€‘\nã‚µãƒ–ã‚¿ã‚¹ã‚¯: {r.get('assigned_subtask', {}).get('task', 'N/A')[:50]}...\nå›ç­”:\n{r['output'][:500]}..."
        for i, r in enumerate(worker_results) if r["success"]
    ])
    
    synthesis_prompt = f"""ã‚ãªãŸã¯æŒ‡æ®è€…ã¨ã—ã¦ã€è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å›ç­”ã‚’çµ±åˆã—ã¾ã™ã€‚

ã€å…ƒã®ã‚¿ã‚¹ã‚¯ã€‘
{request.task}

ã€å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å›ç­”ã€‘
{worker_outputs}

ä¸Šè¨˜ã®å›ç­”ã‚’çµ±åˆã—ã€å…ƒã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ãªæœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã®è‰¯ã„ç‚¹ã‚’æ´»ã‹ã—ã€çŸ›ç›¾ãŒã‚ã‚Œã°è§£æ±ºã—ã¦ãã ã•ã„ã€‚"""
    
    synthesis_result = await loop.run_in_executor(
        None,
        executor.invoke_model,
        request.conductor_model_id,
        synthesis_prompt,
        request.max_tokens * 2,
        request.temperature,
        99
    )
    
    result["phases"].append({
        "phase": "synthesis",
        "conductor_response": synthesis_result
    })
    result["final_answer"] = synthesis_result.get("output", "")
    
    return result


async def _execute_evaluate_mode(request, executor, loop, result):
    """è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰"""
    # å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã«åŒã˜ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã•ã›ã‚‹
    print(f"\nğŸ”§ Phase 1: å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ä¸¦åˆ—å®Ÿè¡Œä¸­...")
    worker_results = await loop.run_in_executor(
        None,
        executor.execute_parallel_models,
        request.worker_model_ids,
        request.task,
        request.max_tokens,
        request.temperature,
        len(request.worker_model_ids)
    )
    
    result["phases"].append({
        "phase": "worker_execution",
        "results": worker_results
    })
    
    # æŒ‡æ®è€…ãŒè©•ä¾¡
    print(f"\nğŸ“Š Phase 2: æŒ‡æ®è€…ã«ã‚ˆã‚‹è©•ä¾¡ä¸­...")
    worker_outputs = "\n\n".join([
        f"ã€å›ç­”{i+1} - {r['model_id'].split('.')[-1][:20]}ã€‘\n{r['output']}"
        for i, r in enumerate(worker_results) if r["success"]
    ])
    
    eval_prompt = f"""ã‚ãªãŸã¯å¯©æŸ»å“¡ã¨ã—ã¦ã€è¤‡æ•°ã®AIãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

ã€ã‚¿ã‚¹ã‚¯ã€‘
{request.task}

ã€å„ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã€‘
{worker_outputs}

ä»¥ä¸‹ã®è¦³ç‚¹ã§å„å›ç­”ã‚’è©•ä¾¡ã—ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. æ­£ç¢ºæ€§
2. å®Œå…¨æ€§
3. æ˜ç¢ºã•
4. å®Ÿç”¨æ€§

JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
```json
{{
  "ranking": [
    {{"rank": 1, "model": "ãƒ¢ãƒ‡ãƒ«å", "score": 95, "strengths": ["å¼·ã¿1"], "weaknesses": ["å¼±ã¿1"]}},
    ...
  ],
  "best_answer_summary": "æœ€ã‚‚å„ªã‚ŒãŸå›ç­”ã®è¦ç´„",
  "overall_analysis": "å…¨ä½“çš„ãªåˆ†æ"
}}
```"""
    
    eval_result = await loop.run_in_executor(
        None,
        executor.invoke_model,
        request.conductor_model_id,
        eval_prompt,
        request.max_tokens * 2,
        0.3,
        99
    )
    
    result["phases"].append({
        "phase": "evaluation",
        "conductor_response": eval_result
    })
    result["final_answer"] = eval_result.get("output", "")
    
    return result


async def _execute_synthesize_mode(request, executor, loop, result):
    """çµ±åˆãƒ¢ãƒ¼ãƒ‰"""
    # å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã«åŒã˜ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã•ã›ã‚‹
    print(f"\nğŸ”§ Phase 1: å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ä¸¦åˆ—å®Ÿè¡Œä¸­...")
    worker_results = await loop.run_in_executor(
        None,
        executor.execute_parallel_models,
        request.worker_model_ids,
        request.task,
        request.max_tokens,
        request.temperature,
        len(request.worker_model_ids)
    )
    
    result["phases"].append({
        "phase": "worker_execution",
        "results": worker_results
    })
    
    # æŒ‡æ®è€…ãŒçµ±åˆ
    print(f"\nğŸ¯ Phase 2: å›ç­”çµ±åˆä¸­...")
    worker_outputs = "\n\n".join([
        f"ã€{r['model_id'].split('.')[-1][:25]}ã®å›ç­”ã€‘\n{r['output']}"
        for r in worker_results if r["success"]
    ])
    
    synth_prompt = f"""ã‚ãªãŸã¯è¤‡æ•°ã®AIãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’çµ±åˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

ã€ã‚¿ã‚¹ã‚¯ã€‘
{request.task}

ã€å„ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã€‘
{worker_outputs}

ä¸Šè¨˜ã®å…¨ã¦ã®å›ç­”ã‹ã‚‰æœ€è‰¯ã®è¦ç´ ã‚’æŠ½å‡ºã—ã€çµ±åˆã•ã‚ŒãŸæœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
- å„å›ç­”ã®å„ªã‚ŒãŸç‚¹ã‚’æ´»ã‹ã™
- çŸ›ç›¾ã™ã‚‹æƒ…å ±ã¯æœ€ã‚‚ä¿¡é ¼æ€§ã®é«˜ã„ã‚‚ã®ã‚’æ¡ç”¨
- ä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±ãŒã‚ã‚Œã°è£œå®Œ
- èª­ã¿ã‚„ã™ãæ§‹é€ åŒ–ã•ã‚ŒãŸå½¢å¼ã§å‡ºåŠ›"""
    
    synth_result = await loop.run_in_executor(
        None,
        executor.invoke_model,
        request.conductor_model_id,
        synth_prompt,
        request.max_tokens * 2,
        request.temperature,
        99
    )
    
    result["phases"].append({
        "phase": "synthesis",
        "conductor_response": synth_result
    })
    result["final_answer"] = synth_result.get("output", "")
    
    return result
